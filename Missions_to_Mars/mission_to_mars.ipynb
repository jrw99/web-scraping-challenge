{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WDM] - \n",
      "\n",
      "[WDM] - ====== WebDriver manager ======\n",
      "[WDM] - Current google-chrome version is 95.0.4638\n",
      "[WDM] - Get LATEST driver version for 95.0.4638\n",
      "[WDM] - Driver [C:\\Users\\Jon\\.wdm\\drivers\\chromedriver\\win32\\95.0.4638.54\\chromedriver.exe] found in cache\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "executable_path = {'executable_path': ChromeDriverManager().install()}\n",
    "browser = Browser(\"chrome\", **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NASA Mars News\n",
    "\n",
    "Scrape the Mars News Site and collect the latest News Title and Paragraph Text. Assign the text to variables that you can reference later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NASA to Broadcast Mars 2020 Perseverance Launch, Prelaunch Activities\n",
      "Starting July 27, news activities will cover everything from mission engineering and science to returning samples from Mars to, of course, the launch itself.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the page\n",
    "url = 'https://redplanetscience.com/'\n",
    "browser.visit(url)\n",
    "\n",
    "# Scrape into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# get the first list_date (if needed) and retain content_title and article_teaser_body inside of list_text as they are in chronological order with the latest being first\n",
    "# MarsNews_latest_date = soup.find('div', class_='list_date').get_text()\n",
    "MarsNews_latest_Title = soup.find('div', class_='content_title').get_text()\n",
    "MarsNews_latest_ArticleBody = soup.find('div', class_='article_teaser_body').get_text()\n",
    "print(MarsNews_latest_Title + '\\n' + MarsNews_latest_ArticleBody)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JPL Mars Space Images - Featured Image\n",
    "\n",
    "Use splinter to navigate the site and find the image url for the current Featured Mars Image and assign the url string to a variable called featured_image_url.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://spaceimages-mars.com/image/featured/mars1.jpg'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Load the page\n",
    "url = 'https://spaceimages-mars.com/'\n",
    "browser.visit(url)\n",
    "\n",
    "# Scrape into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# Make sure to find the image url to the full size .jpg image.\n",
    "btn = soup.find('button', text=' FULL IMAGE')\n",
    "if btn.parent.name == 'a':\n",
    "    a = btn.parent\n",
    "\n",
    "if a:\n",
    "    featured_image_url = a['href']\n",
    "\n",
    "# Make sure to save a complete url string for this image.\n",
    "featured_image_url = url + featured_image_url\n",
    "featured_image_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Facts\n",
    "\n",
    "Visit the Mars Facts webpage here and use Pandas to scrape the table containing facts about the planet including Diameter, Mass, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<table border=\"1\" class=\"dataframe\">  <thead>    <tr style=\"text-align: right;\">      <th></th>      <th>0</th>      <th>1</th>    </tr>  </thead>  <tbody>    <tr>      <th>0</th>      <td>Equatorial Diameter:</td>      <td>6,792 km</td>    </tr>    <tr>      <th>1</th>      <td>Polar Diameter:</td>      <td>6,752 km</td>    </tr>    <tr>      <th>2</th>      <td>Mass:</td>      <td>6.39 × 10^23 kg (0.11 Earths)</td>    </tr>    <tr>      <th>3</th>      <td>Moons:</td>      <td>2 ( Phobos &amp; Deimos )</td>    </tr>    <tr>      <th>4</th>      <td>Orbit Distance:</td>      <td>227,943,824 km (1.38 AU)</td>    </tr>    <tr>      <th>5</th>      <td>Orbit Period:</td>      <td>687 days (1.9 years)</td>    </tr>    <tr>      <th>6</th>      <td>Surface Temperature:</td>      <td>-87 to -5 °C</td>    </tr>    <tr>      <th>7</th>      <td>First Record:</td>      <td>2nd millennium BC</td>    </tr>    <tr>      <th>8</th>      <td>Recorded By:</td>      <td>Egyptian astronomers</td>    </tr>  </tbody></table>'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use Pandas to convert the data to a HTML table string.\n",
    "\n",
    "# scrape the site and gather the html for the table\n",
    "url = 'https://galaxyfacts-mars.com/'\n",
    "table = pd.read_html(url, match='Equatorial Diameter')\n",
    "\n",
    "# convert to dataframe\n",
    "df = table[0]\n",
    "\n",
    "# render out to html table string\n",
    "html_tbl = df.to_html()\n",
    "html_tbl = html_tbl.replace('\\n', '')\n",
    "html_tbl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mars Hemispheres\n",
    "\n",
    "Visit the astrogeology site here to obtain high resolution images for each of Mar's hemispheres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'title': 'Cerberus Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/cerberus_enhanced.tif'},\n",
       " {'title': 'Schiaparelli Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/schiaparelli_enhanced.tif'},\n",
       " {'title': 'Syrtis Major Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/syrtis_major_enhanced.tif'},\n",
       " {'title': 'Valles Marineris Hemisphere Enhanced',\n",
       "  'img_url': 'https://marshemispheres.com/images/valles_marineris_enhanced.tif'}]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hemisphere_image_urls = []\n",
    "\n",
    "# load initial page\n",
    "url = 'https://marshemispheres.com/'\n",
    "browser.visit(url)\n",
    "\n",
    "# Scrape into Soup\n",
    "html = browser.html\n",
    "soup = bs(html, 'html.parser')\n",
    "\n",
    "# Second browser to find full res image in sub pages\n",
    "browser2 = Browser(\"chrome\", **executable_path, headless=False)\n",
    "\n",
    "# find the parent of the list of hemishpere items we want\n",
    "container = soup.find('div', class_='results')\n",
    "\n",
    "# get all of the hemispheres within\n",
    "items = container.find_all('div', class_='item')\n",
    "\n",
    "# loop through each hemishpere \n",
    "for item in items:\n",
    "   \n",
    "    # get the title \n",
    "    title = item.find('h3').get_text() \n",
    "\n",
    "    # launch second browser to sub page to find the full res image\n",
    "    page = item.find('a', class_='product-item')['href']\n",
    "    url2 = url + page   \n",
    "    browser2.visit(url2)\n",
    "    html2 = browser2.html\n",
    "    soup2 = bs(html2, 'html.parser')\n",
    "\n",
    "    img_url = url + soup2.find('a', text='Original')['href']\n",
    "\n",
    "    # Save both the image url string for the full resolution hemisphere image, and the Hemisphere title containing the hemisphere name. Use a Python dictionary to store the data using the keys img_url and title.\n",
    "    hemishpere_dict = {\n",
    "        \"title\":title, \n",
    "        \"img_url\":img_url\n",
    "    }\n",
    "\n",
    "    # Append the dictionary with the image url string and the hemisphere title to a list. This list will contain one dictionary for each hemisphere.\n",
    "    hemisphere_image_urls.append(hemishpere_dict)\n",
    "\n",
    "browser2.quit()\n",
    "browser.quit()\n",
    "\n",
    "hemisphere_image_urls"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6f919533ec28e57a5a3906a98c051a51369577d2c8877752684df5cc6307b020"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('PythonBootcamp': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
